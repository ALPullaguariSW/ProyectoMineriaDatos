\chapter{Metodología}

Este proyecto implementa un pipeline de datos completo siguiendo las fases de SEMMA. A continuación se detalla la ejecución técnica de cada fase.

\section{Fase 1: Sample (Muestreo y Adquisición)}
El objetivo fue construir un dataset masivo que representara la realidad del desarrollo de software.
\subsection{Minería de Repositorios (Repo Miner)}
Se desarrolló el script \texttt{repo\_miner.py} que automatiza la recolección de datos:
\begin{enumerate}
    \item \textbf{Clonado}: Descarga repositorios de alto perfil (Linux Kernel, React, Django, TensorFlow) para asegurar diversidad de lenguajes (C, JS, Python).
    \item \textbf{Filtrado}: Descarta archivos irrelevantes (imágenes, binarios, documentación) y archivos demasiado pequeños ($<50$ bytes) o gigantes ($>100$KB) que podrían ser ruido.
    \item \textbf{Etiquetado Automático (Weak Supervision)}:
    Se implementó una función de "oráculo" basada en Reglas (\texttt{preprocessing.get\_dangerous\_details}). Si un archivo contiene patrones regex conocidos de vulnerabilidades (ej: \texttt{strcpy} sin validación, inyecciones SQL crudas), se etiqueta preliminarmente como \texttt{is\_vulnerable=1}. De lo contrario, se asume \texttt{0}.
    Esto permitió generar un dataset de **186,958 muestras** sin intervención humana manual.
\end{enumerate}

\section{Fase 2: Explore (Exploración)}
Mediante scripts de EDA (\texttt{eda.py}), se analizaron las propiedades del dataset minado:
\begin{itemize}
    \item \textbf{Desbalance de Clases}: Se observó una proporción de 20:1 a favor del código seguro. Entrenar con esto causaría un modelo sesgado que siempre predice "Seguro".
    \item \textbf{Longitud de Código}: La mayoría de las funciones vulnerables tenían una longitud media, lo que sugiere que la complejidad no siempre implica longitud, sino estructura.
\end{itemize}

\section{Fase 3: Modify (Modificación y Preprocesamiento)}
Esta fase fue crítica para limpiar la señal de los datos. El script \texttt{preprocessing.py} realiza:
\subsection{Limpieza de Texto}
\begin{itemize}
    \item Eliminación de comentarios (que no afectan la ejecución pero añaden ruido al NLP).
    \item Normalización de espacios y saltos de línea.
\end{itemize}
\subsection{Ingeniería de Características (Feature Extraction)}
Se construyó un vector de características híbrido para cada archivo:
\begin{enumerate}
    \item \textbf{Vector TF-IDF (1000 dimensiones)}: Se seleccionaron los 1000 tokens más relevantes (unigramas y bigramas) del corpus. Esto captura la semántica del código (ej: \texttt{SELECT * FROM} + \texttt{input}).
    \item \textbf{Características de Dominio}:
    \begin{itemize}
        \item \texttt{cyclomatic\_complexity}: Calculada contando estructuras de control (\texttt{if}, \texttt{for}, \texttt{while}).
        \item \texttt{loc}: Líneas de código efectivas.
        \item \texttt{dangerous\_calls\_count}: Frecuencia de uso de APIs peligrosas conocidas.
    \end{itemize}
\end{enumerate}
\subsection{Balanceo Inteligente (Smart Polishing)}
Para corregir el desbalance detectado en la fase de Exploración, se aplicó una técnica de **Submuestreo Aleatorio (Random Undersampling)** en la clase mayoritaria (Seguro) hasta igualar la cantidad de muestras de la clase minoritaria (Vulnerable). Esto asegura que el modelo preste la misma atención a ambas clases.

\section{Fase 4: Model (Modelado)}
Se utilizó \texttt{train\_model.py} para entrenar los clasificadores.
\subsection{Configuración de Random Forest}
Se empleó \textbf{GridSearchCV} para una búsqueda exhaustiva de hiperparámetros:
\begin{itemize}
    \item \texttt{n\_estimators}: [50, 100, 200] (Número de árboles). Más árboles estabilizan la predicción.
    \item \texttt{max\_depth}: [None, 10, 20] (Profundidad máxima). Limitar la profundidad ayuda a prevenir el sobreajuste.
    \item \texttt{criterion}: ['gini', 'entropy']. Métrica para medir la calidad de la división en los nodos.
\end{itemize}
El mejor modelo resultante fue un Random Forest con 100 árboles y profundidad ilimitada, lo que indica que la complejidad de los patrones de vulnerabilidad requiere modelos profundos.

\section{Fase 5: Assess (Evaluación)}
Los modelos se evaluaron utilizando un conjunto de prueba (20\% de los datos) que nunca fue visto durante el entrenamiento. Se generaron métricas de precisión, recall, F1-score y matrices de confusión. Además, se analizaron las curvas de aprendizaje para detectar problemas de sobreajuste (overfitting).

\section{Fase 6: Despliegue y Automatización (CI/CD)}
Para operacionalizar el modelo y cumplir con los principios de DevSecOps, se implementó un pipeline de Integración Continua utilizando **GitHub Actions**.

\subsection{Arquitectura del Pipeline}
El flujo de trabajo (\texttt{security\_scan.yml}) se activa automáticamente ante cada \textit{push} o \textit{pull request} al repositorio. Consta de las siguientes etapas:

\begin{enumerate}
    \item \textbf{Setup}: Configuración del entorno Python y recuperación de dependencias.
    \item \textbf{Static Analysis}: Ejecución del escáner (\texttt{scan\_repo.py}) que utiliza el modelo entrenado para analizar los archivos modificados.
    \item \textbf{Blocking Mechanism}: Si el modelo detecta vulnerabilidades con una probabilidad superior al umbral definido (ej: 80\%), el pipeline falla automáticamente, impidiendo que código inseguro llegue a producción.
    \item \textbf{Reporting}: Generación de un reporte HTML detallado con las vulnerabilidades halladas, sus IDs de CWE/OWASP y sugerencias de remediación.
\end{enumerate}

\subsection{Containerización}
Se creó un \texttt{Dockerfile} para encapsular el entorno de ejecución, asegurando que el escáner funcione de manera consistente en cualquier servidor de integración o máquina de desarrollador, eliminando problemas de dependencias ("it works on my machine").
